{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Campus Contributor Documentation","text":"<p>These pages are intended for current and potential campus contributors to the Open Science Data Federation (OSDF) and Open Science Pool (OSPool), including cluster administrators, project PIs and leadership, and campus staff who facilitate the use of clusters.</p> <p>Please let us know how we can improve these pages to better support you!</p>"},{"location":"#open-science-data-federation-osdf","title":"Open Science Data Federation (OSDF)","text":"<p>The Open Science Data Federation (OSDF) connects disparate dataset repositories into a single, nationwide data distribution network.</p> <p>At this time, information about contributing to the OSDF is located on our main OSDF webpage.</p>"},{"location":"#open-science-pool-ospool","title":"Open Science Pool (OSPool)","text":"<p>The Open Science Pool (OSPool) presents a unified means of accessing computing capacity that is shared from campuses nationwide via a single HTCondor pool. This pool is accessible to any researcher affiliated with a US academic institution.</p> <p>Want to contribute to the OSPool from your campus?</p> <p>Get started on our OSPool integration page.</p> <p>Already contributing to the OSPool?</p> <ul> <li>Learn more about your contributions and how researchers benefit from them</li> <li>See how to investigate live OSPool jobs on your system</li> <li>Check our list of suggested maintenance tasks and when to contact us about the integration</li> </ul>"},{"location":"#community","title":"Community","text":"<p>Participate in the community of OSDF and OSPool campus contributors!</p> <p>There are several ways to participate:</p> <ul> <li> <p>Join our Campus Contributors mailing list (a Google Group).     We send one or two emails per week, mostly just announcements of the Campus Meet-Ups.     Email us with email addresses to add.</p> </li> <li> <p>Participate in the weekly Campus Meet-Ups on Zoom,     Wednesdays at 11:00am to noon, US Central Time.     The virtual meeting link is included in email announcements,     or email us for the link.</p> </li> <li> <p>Join our OSG Consortium Slack Workspace;     email us with email addresses to invite.</p> </li> <li> <p>Attend our annual Throughput Computing Week event.     HTC26     will be held 9\u201312 June 2026 at the University of Wisconsin\u2013Madison.</p> </li> <li> <p>If suitable, apply to join     the OSG School.</p> </li> </ul>"},{"location":"ospool-integration/","title":"OSPool Integration Process","text":"<p>This page explains the process for integrating your compute cluster with the OSPool.</p>"},{"location":"ospool-integration/#overall-process","title":"Overall Process","text":"<p>We organize the integration process around two meetings, plus some tasks in-between and after:</p> <ol> <li> <p>Start. Just email support@osg-htc.org and     write that you want to contribute to the OSPool!     If you want, give an overview of     who is contributing,     your goals,     and your computing system     (detailed technical specs are not needed).     We aim to reply quickly and get started.</p> </li> <li> <p>Kickoff meeting.     We will discuss in more depth your goals for the project and what you aim to contribute,     plus the technical options (see below) for integration and the remaining process.     The project PI or lead, and participating technical lead and staff, are encouraged to join;     the meeting lasts about 1 hour.</p> </li> <li> <p>Preparations.     After the kickoff, both teams will have preparatory steps to complete;     these items will be detailed in a follow-up email.</p> </li> <li> <p>Live integration session.     When everyone is ready,     we schedule a joint, live integration session to     verify site readiness,     complete the integration,     monitor the first OSPool jobs running via the integration, and     plan for next steps.     Certain technical staff from both teams are required,     and we allocate 2 hours for the session.</p> <p>This session is live to avoid the inevitable and sometimes lengthy delays that can arise when trying to troubleshoot technical issues over email. In our experience, most integrations need just one 2-hour session.</p> </li> <li> <p>Post-integration follow-up activities.     Once the integration is working,     there is a small list of follow-up activities to complete:     verifying the configuration of contributions,     scaling up,     accessing views of the contributions and how they are used, and     setting up communication for ongoing maintenance and community.</p> </li> </ol> <p>Throughout the process, you remain in control of the overall schedule. If you want to move fast, so do we! If you are juggling a busy schedule, we will move forward when you have time.</p>"},{"location":"ospool-integration/#integration-mechanisms","title":"Integration Mechanisms","text":"<p>We offer two fundamental ways to integrate some of your computing capacity with the OSPool, and the first way has two flavors for a total of three options. These are outlined below.</p> <p>We will discuss the options and their tradeoffs during the kickoff meeting (see above), so consider the information below as a starting point.</p> <ul> <li> <p>Option 1a: OSG Hosted CE</p> <p>If you manage your computing capacity with a batch system (e.g., Slurm, HTCondor), then we can provide an automated service (an HTCondor-CE) to submit jobs into your batch system and manage them. In this option, we offer to host and operate your HTCondor-CE service for you in one of our data centers.</p> <p>The jobs submitted to your batch system are not researcher jobs; instead, each one is a glidein job that, when run, creates an HTCondor runtime environment called an Execution Point, which will in turn join the OSPool and accept researcher jobs from it.</p> <p>The Hosted CE service submits and manages jobs via SSH to an unprivileged account on a login node that you provide. You can read detailed requirements for this mechanism and we will discuss them during the kickoff.</p> </li> <li> <p>Option 1b: Self-Operated CE</p> <p>In this variant of the HTCondor-CE mechanism, you host and operate the HTCondor-CE service yourself. Then, you provide access from our OSPool central services to that HTCondor-CE service, so that we can send glidein jobs to your CE.</p> <p>The tradeoffs and requirements for this option differ from the Hosted CE option, and we can discuss those during the kickoff.</p> </li> <li> <p>Option 2: Containers</p> <p>You can also use containers to contribute capacity to the OSPool. For this option, we provide a container image, and you run it on one or more hosts in your cluster. Each running container creates the same kind of Execution Point as above, and thus joins the OSPool with the capacity given to the container.</p> <p>This option may be most suitable if you already manage your computing capacity with containers or have reasons to avoid sharing some capacity using a batch system.</p> </li> </ul>"},{"location":"ospool-maintenance/","title":"OSPool Integration Maintenance","text":"<p>In general, once your cluster is contributing to the OSPool, things should mostly just continue to work. There are no required, routine maintenance tasks specific to maintaining the integration.</p>"},{"location":"ospool-maintenance/#when-to-contact-us","title":"When To Contact Us","text":"<p>If you have or are concerned about a possible security incident that affects your cluster or may affect our integration, please email our cybersecurity team.</p> <p>For all other topics, please email our support system; a real human will reply ASAP!</p> <p>Some reasons to contact us:</p> <ul> <li> <p>You have a scheduled or unplanned downtime.     Please see here     for instructions on registering that downtime with us;     we will stop trying to submit jobs into your cluster until it is over     (or you tell us explicitly to start again).</p> </li> <li> <p>Your site changes in a way that could affect the integration     (see the Requirements page);     e.g., networking, login node, SSH, cluster configuration, OS distro or major version.</p> </li> <li> <p>You want to change the configuration (\u201cshape\u201d or maximum number)     of jobs that we submit into your cluster.</p> </li> </ul>"},{"location":"ospool-maintenance/#monitoring","title":"Monitoring","text":"<p>If you want to monitor aspects of the integration, here are a few ideas:</p> <ul> <li> <p>Verify that SSH to the login node continues to work.</p> </li> <li> <p>Verify that the batch system on the login node is working;     for example, can our user run basic query and submit commands?</p> </li> <li> <p>Check whether any of our jobs run, especially when you would expect them to.</p> </li> <li> <p>If your integration is via a Hosted CE (Option 1a),     check your OSPool Hosted CE Dashboard     to see if the OSPool is actually getting your contributions and allocating capacity to researchers.</p> </li> <li> <p>If you told us to configure a Squid proxy service,     perform a periodic functional test on it:     Can it actually fetch a page from offsite?     Squid has a strange failure mode in which the service is running     but will not respond to any attempt to actually use it.</p> </li> </ul>"},{"location":"ospool-metrics/","title":"OSPool Metrics and Monitoring","text":"<p>Once you are contributing to the OSPool, you may be interested in details about those contributions.</p>"},{"location":"ospool-metrics/#viewing-metrics-about-contributions-and-their-usage","title":"Viewing Metrics About Contributions and Their Usage","text":"<p>After your OSPool integration is working, we will send you links that can help you see how your contributions are going and how they are being allocated to researchers.</p> <p>In general, we offer:</p> <ul> <li> <p>OSPool (Hosted) CE Dashboard</p> <p>If you are contributing via a Hosted CE (Option 1a), there will be a CE Dashboard view for your CE and site. We will send you the link to that page during the post-integration follow-up.</p> <p>On the top of the page, there are small charts that show resource contributions (top, red line) and usage (blue area) for CPUs, memory, and disk (BCUs are a mess right now). If you scroll down, there is detailed information about the science projects that benefitted from running on your cluster; in the table of projects, you can click a project name to get details about it.</p> </li> <li> <p>OSPool Contributors</p> <p>There is an overview page for all OSPool contributors, showing data from the past year. You can click on your campus name to get more details. This may be a good resource for, say, annual reporting.</p> </li> <li> <p>OSPool Projects</p> <p>In the previous view, projects are not linked to their project details pages. Instead, there is an overview page for all OSPool Projects. Click on the name of a project to learn more about it.</p> </li> <li> <p>Map</p> <p>Visit our map of OSPool Contributors; click a marker to view details about that site. You can do usual map things (moving, zooming in and out, etc.) and capture the resulting map with the \u201cPrint as PNG\u201d button.</p> </li> </ul> <p>Were you hoping or looking for other kinds of information from us, or the same kinds of information but presented differently? Let us know how we can improve!</p>"},{"location":"ospool-metrics/#investigating-live-ospool-jobs","title":"Investigating Live OSPool Jobs","text":"<p>In addition to your usual operating system and batch system tools, and the OSPool metrics above, we provide some ways to investigate live OSPool jobs. You are welcome to use these tools, and we may ask you to use them when troubleshooting.</p>"},{"location":"ospool-metrics/#viewing-researcher-jobs-running-within-a-glidein-job","title":"Viewing researcher jobs running within a glidein job","text":"<ol> <li> <p>Log in to a worker node as the OSPool user (e.g., \u201cosg01\u201d but may be different at your site)</p> </li> <li> <p>Pick a glidein (and its directory):</p> <p>Note: SCRATCH is the path of the scratch directory you gave us to put glideins in.</p> <ol> <li> <p>Option 1: List HTCondor processes, pick one, and note its glidein directory:</p> <pre><code>$ ps -u osg01 -f | grep master\nosg01    4122304 [...] SCRATCH/glide_qJDh7z/main/condor/sbin/condor_master [...]\n</code></pre> </li> <li> <p>Option 2: List glidein directories and pick an active one:</p> <pre><code>$ ls -l SCRATCH/glide_*/_GLIDE_LEASE_FILE\n</code></pre> <p>Pick a \u201cglide_xxxxxx\u201d directory that has a lease file with a timestamp less than about 5 minutes old.</p> </li> </ol> <p>Note: Let GLIDEDIR be the path to the chosen glidein directory, e.g., <code>SCRATCH/glide_xxxxxx</code></p> </li> <li> <p>Make sure HTCondor is recent enough:</p> <pre><code>$ GLIDEDIR/main/condor/bin/condor_version\n$CondorVersion: 24.6.1 2025-03-20 BuildID: 794846 PackageID: 24.6.1-1 $\n$CondorPlatform: x86_64_AlmaLinux9 $\n</code></pre> <p>The Condor Version should be 2.7.0 or later; if not, go back to step 2 and pick a different glidein directory.</p> </li> <li> <p>Pick the PID of an HTCondor \u201cstartd\u201d process to query:</p> <ol> <li> <p>If you are just exploring, run the following command and pick any \u201ccondor_startd\u201d process \u2014 it does not matter if it is associated with the HTCondor instance identified in steps 2\u20133 above:</p> <pre><code>$ ps -u osg01 -f | grep condor_startd\n</code></pre> </li> <li> <p>If you are looking for the \u201cstartd\u201d associated with some other process, start with a process tree:</p> <pre><code>$ ps -u osg01 -f --forest\n</code></pre> <p>Find the process of interest, then work upward in the process tree to the nearest ancestor \u201ccondor_startd\u201d process.</p> </li> <li> <p>In either case, note the PID (leftmost number) on the \u201ccondor_startd\u201d line you picked.</p> </li> </ol> </li> <li> <p>Run <code>condor_who</code> on the PID you picked:</p> <pre><code>$ GLIDEDIR/main/condor/bin/condor_who -pid PID -ospool\n&lt;b&gt;Batch System : SLURM&lt;/b&gt;\n&lt;b&gt;Batch Job    : 346675&lt;/b&gt;\n&lt;b&gt;Birthdate    : 2025-04-07 14:47:02&lt;/b&gt;\n&lt;b&gt;Temp Dir     : /var/lib/condor/execute/osg01/glide_qJDh7z/tmp&lt;/b&gt;\n&lt;b&gt;Startd : glidein_4101601_324283152@spark-a220.chtc.wisc.edu has 1 job(s) running:&lt;/b&gt;\nPROJECT   USER   AP_HOSTNAME         JOBID        RUNTIME    MEMORY    DISK      CPUs EFCY PID     STARTER\nInst-Proj jsmith ap20.uc.osg-htc.org 27781234.0   0+00:17:43  512.0 MB  129.0 MB    1 0.00 4124321 4123123\n\u2026\n</code></pre> </li> </ol> <p>Definitions of fields in the header (before the PROJECT USER \u2026 row):</p> Batch System HTCondor\u2019s name for the type of batch system you are running. Batch Job The identifier for this glidein job in your batch system. Birthdate When HTCondor began running within this glidein; typically, this is a few minutes after the glidein job itself began running. Temp Dir The path to the glidein job directory (remove the trailing \u201c/tmp\u201d) Startd HTCondor\u2019s identifier for its \u201cstartd\u201d process within the glidein job. <p>Definitions of fields in each row of the researcher job table:</p> PROJECT The OSPool project identifier for this researcher job USER The OSPool AP\u2019s user identifier for this researcher job AP_HOSTNAME The OSPool AP\u2019s hostname JOBID HTCondor\u2019s identifier for this researcher job on its AP RUNTIME HTCondor\u2019s value for the runtime of the researcher job MEMORY The amount of memory (RAM), in MB, that HTCondor allocated to this researcher job DISK The amount of disk, in KB, that HTCondor allocated to this researcher job CPUs The number of CPU cores that HTCondor allocated to this researcher job EFCY An HTCondor measure of the efficiency of the job, roughly calculated as CPU time / wallclock time; a value noticeably greater than the CPUs value may mean the researcher job is using more cores than requested PID The local PID of the researcher job, or more often, of the root of the process tree for the researcher job (e.g., this could be a wrapper script or even Singularity or Apptainer for a researcher job in a container) STARTER The local PID of the HTCondor \u201cstarter\u201d process that owns this research job"},{"location":"ospool-metrics/#viewing-researcher-jobs-running-within-all-ospool-glidein-jobs","title":"Viewing researcher jobs running within all OSPool glidein jobs","text":"<p>Note: Steps 1\u20133 are the same as above.</p> <ol> <li> <p>Log in to a worker node as the OSPool user (e.g., \u201cosg01\u201d but may be different at your site)</p> </li> <li> <p>Pick a glidein (and its directory):</p> <p>Note: SCRATCH is the path of the scratch directory you gave us to put glideins in.</p> <ol> <li>Option 1: List HTCondor processes, pick one, and note its glidein directory:</li> </ol> <pre><code>$ ps -u osg01 -f | grep master\nosg01    4122304 [...] SCRATCH/glide_qJDh7z/main/condor/sbin/condor_master [...]\n</code></pre> <ol> <li>Option 2: List glidein directories and pick an active one:</li> </ol> <pre><code>$ ls -l SCRATCH/glide_*/_GLIDE_LEASE_FILE\n</code></pre> <p>Pick a \u201cglide_xxxxxx\u201d directory that has a lease file with a timestamp less than about 5 minutes old.</p> <p>Note: Let GLIDEDIR be the path to the chosen glidein directory, e.g., <code>SCRATCH/glide_xxxxxx</code></p> </li> <li> <p>Make sure HTCondor is recent enough:</p> <pre><code>$ GLIDEDIR/main/condor/bin/condor_version\n$CondorVersion: 24.6.1 2025-03-20 BuildID: 794846 PackageID: 24.6.1-1 $\n$CondorPlatform: x86_64_AlmaLinux9 $\n</code></pre> <p>The Condor Version should be 2.7.0 or later; if not, go back to step 2 and pick a different glidein directory.</p> </li> <li> <p>Run <code>condor_who</code> on all discoverable glideins on this host running as the current user:</p> <pre><code>$ GLIDEDIR/main/condor/bin/condor_who -allpids -ospool\n\nBatch System : SLURM\nBatch Job    : 346675\nBirthdate    : 2025-04-07 14:47:02\nTemp Dir     : /var/lib/condor/execute/osg01/glide_qJDh7z/tmp\nStartd : glidein_4101601_324283152@spark-a220.chtc.wisc.edu has 1 job(s) running:\nPROJECT   USER   AP_HOSTNAME         JOBID        RUNTIME    MEMORY    DISK      CPUs EFCY PID     STARTER\nInst-Proj jsmith ap20.uc.osg-htc.org 27781234.0   0+00:17:43  512.0 MB  129.0 MB    1 0.00 4124321 4123123\n\u2026\n</code></pre> </li> </ol> <p>For each glidein job, there will be one set of heading lines (\u201cBatch System\u201d, etc.) and a table of researcher jobs, one per line; format and definitions are as above.</p> <p>OSPool Contribution Requirements</p>"},{"location":"ospool-metrics/#contributing-via-a-hosted-ce","title":"Contributing via a Hosted CE","text":"<ul> <li> <p>The cluster and login node are set up for our user account:</p> </li> <li> <p>The cluster is operational and generally works</p> </li> <li> <p>The user account has a home directory on the login node</p> </li> <li> <p>The user account can read, write, and execute files and directories within its home directory</p> </li> <li> <p>Our home directory has enough available space and inodes (TBD but not a lot)</p> </li> <li> <p>PATH staff know the right partition (and other batch system config) to use</p> </li> <li> <p>The batch system is configured to allow the user account to submit jobs to the right partition(s) and for the default job \u201cshape\u201d (e.g., 1 core, 2 GB memory, and 24-hour maximum run time)</p> </li> <li> <p>It is possible to SSH from the CE to the login node:</p> </li> <li> <p>PATh staff know the current hostname of your login node</p> </li> <li> <p>That hostname has a public DNS entry that resolves to the correct IP address</p> </li> <li> <p>PATh staff know the user account name (default, \u201cosg01\u201d)</p> </li> <li> <p>PATh staff know about SSH configuration details to use (e.g., alternate port, jump host)</p> </li> <li> <p>The SSH client on one of our IP addresses can connect to your login node (through firewalls, etc.)</p> </li> <li> <p>The provided SSH public key has been installed in the right place and with the right permissions</p> </li> <li> <p>The provided SSH public key is sufficient for authentication by your SSH server</p> </li> <li> <p>The worker nodes on which our jobs may run are ready:</p> </li> <li> <p>Our home directory is shared with each cluster node</p> </li> <li> <p>PATh staff know the correct path to scratch space for jobs (ideally on each worker node, but a shared filesystem may work)</p> </li> <li> <p>Our user account can create subdirectories and run executables in the scratch directory</p> </li> <li> <p>The worker nodes have permissive outbound network connectivity to the Internet (default allow, please note specific restrictions)</p> </li> </ul>"},{"location":"ospool-requirements/","title":"OSPool Contribution Requirements","text":""},{"location":"ospool-requirements/#supported-cluster-oses-and-htcondor-versions","title":"Supported Cluster OSes and HTCondor Versions","text":"<p>In the table below, the OS is what you provide on the login and worker nodes; the HTCondor version is what we will provide via our services.</p> OS HTCondor Notes Enterprise Linux 7\u00a0(*) 23.10.* Enterprise Linux 7 is no longer supported, and thus our ability to support such systems may be removed at any time. Enterprise\u00a0Linux\u00a08\u00a0(*) 25.*\u00a0(&gt;\u00a025.0) Enterprise\u00a0Linux\u00a09\u00a0(*) 25.*\u00a0(&gt;\u00a025.0) Debian\u00a012\u00a0(bookworm) 25.*\u00a0(&gt;\u00a025.0) Debian\u00a013\u00a0(trixie) 25.*\u00a0(&gt;\u00a025.0) Ubuntu\u00a022.04\u00a0(jammy) 25.*\u00a0(&gt;\u00a025.0) Ubuntu\u00a024.04\u00a0(noble) 25.*\u00a0(&gt;\u00a025.0) (*)\u00a0Tested\u00a0variants\u00a0are\u00a0CentOS Stream,\u00a0Alma,\u00a0and\u00a0Rocky. \u2060"},{"location":"ospool-requirements/#contributing-via-a-hosted-ce","title":"Contributing via a Hosted CE","text":""},{"location":"ospool-requirements/#the-cluster-and-login-node-are-set-up-for-our-user-account","title":"The cluster and login node are set up for our user account","text":"<ul> <li>The cluster is operational and generally works</li> <li>The user account has a home directory on the login node</li> <li>The user account can read, write, and execute files and directories within its home directory</li> <li>Our home directory has enough available space and inodes (TBD but not a lot)</li> <li>PATh staff know the right partition (and other batch system config) to use</li> <li>PATh staff know the correct path to the batch-system binaries</li> <li>The batch system is configured to allow the user account to submit jobs to the right partition(s) and for the default job \u201cshape\u201d (e.g., 1 core, 2 GB memory, and 24-hour maximum run time)</li> </ul>"},{"location":"ospool-requirements/#it-is-possible-to-ssh-from-the-ce-to-the-login-node","title":"It is possible to SSH from the CE to the login node:","text":"<ul> <li>PATh staff know the current hostname of your login node</li> <li>That hostname has a public DNS entry that resolves to the correct IP address</li> <li>PATh staff know the user account name (default, \u201cosg01\u201d)</li> <li>PATh staff know about SSH configuration details to use (e.g., alternate port, jump host)</li> <li>The SSH client on one of our IP addresses can connect to your login node (through firewalls, etc.)</li> <li>The provided SSH public key has been installed in the right place and with the right permissions</li> <li>The provided SSH public key is sufficient for authentication by your SSH server</li> </ul>"},{"location":"ospool-requirements/#the-worker-nodes-on-which-our-jobs-may-run-are-ready","title":"The worker nodes on which our jobs may run are ready:","text":"<ul> <li>Our home directory is shared with each cluster node</li> <li>PATh staff know the correct path to scratch space for jobs (ideally on each worker node, but a shared filesystem may work)</li> <li>Our user account can create subdirectories and run executables in the scratch directory</li> <li>The worker nodes have permissive outbound network connectivity to the Internet (default allow, please note specific restrictions)</li> </ul>"},{"location":"ospool-requirements/#feedback","title":"Feedback","text":"<p>Do you have questions or comments about this page? What could we do better? If so, please email us at support@osg-htc.org!</p>"}]}